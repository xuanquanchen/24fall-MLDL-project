{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download requirement environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import Dataset\n",
    "import prepare_data\n",
    "\n",
    "\n",
    "\n",
    "class RoboticsDataset(Dataset):\n",
    "    def __init__(self, file_names, to_augment=False, transform=None, mode='train', problem_type=None):\n",
    "        self.file_names = file_names\n",
    "        self.to_augment = to_augment\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.problem_type = problem_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file_name = self.file_names[idx]\n",
    "        image = load_image(img_file_name)\n",
    "        mask = load_mask(img_file_name, self.problem_type)\n",
    "\n",
    "        data = {\"image\": image, \"mask\": mask}\n",
    "        augmented = self.transform(**data)\n",
    "        image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
    "\n",
    "        image = image.transpose(2, 0, 1)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            if self.problem_type == 'binary':\n",
    "                return image, torch.tensor(mask, dtype=torch.float32).unsqueeze(0)\n",
    "            else:\n",
    "                return image, torch.tensor(mask, dtype=torch.long)\n",
    "        else:\n",
    "            return image, str(img_file_name)\n",
    "\n",
    "\n",
    "def load_image(path):\n",
    "    img = cv2.imread(str(path))\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "def load_mask(path, problem_type):\n",
    "    if problem_type == 'binary':\n",
    "        mask_folder = 'binary_masks'\n",
    "        factor = prepare_data.binary_factor\n",
    "    elif problem_type == 'parts':\n",
    "        mask_folder = 'parts_masks'\n",
    "        factor = prepare_data.parts_factor\n",
    "    elif problem_type == 'instruments':\n",
    "        factor = prepare_data.instrument_factor\n",
    "        mask_folder = 'instruments_masks'\n",
    "\n",
    "    mask = cv2.imread(str(path).replace('images', mask_folder).replace('jpg', 'png'), 0)\n",
    "\n",
    "    return (mask / factor).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get paths of training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepare_data import data_path\n",
    "import random\n",
    "\n",
    "def get_split(random_seeds=42):\n",
    "    random.seed(random_seeds)\n",
    "    train_path = data_path / 'cropped_train'\n",
    "    \n",
    "    train_file_names = []\n",
    "    val_file_names = []\n",
    "\n",
    "    for instrument_id in range(1, 9):\n",
    "        all_file_names = list((train_path / ('instrument_dataset_' + str(instrument_id)) / 'images').glob('*'))\n",
    "        random.shuffle(all_file_names)\n",
    "        split_idx = int(len(all_file_names) * 0.8)\n",
    "        train_file_names.extend(all_file_names[:split_idx])\n",
    "        val_file_names.extend(all_file_names[split_idx:])\n",
    "\n",
    "    return train_file_names, val_file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_crop_height = 1024\n",
    "train_crop_width = 1280\n",
    "val_crop_height = 1024\n",
    "val_crop_width = 1280\n",
    "workers = 12\n",
    "batch_size = 8\n",
    "problem_type = 'parts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def make_loader(file_names, shuffle=False, transform=None, problem_type=problem_type, batch_size=1):\n",
    "        return DataLoader(\n",
    "            dataset=RoboticsDataset(file_names, transform=transform, problem_type=problem_type),\n",
    "            shuffle=shuffle,\n",
    "            num_workers=workers,\n",
    "            batch_size=batch_size,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    HorizontalFlip,\n",
    "    VerticalFlip,\n",
    "    Normalize,\n",
    "    Compose,\n",
    "    PadIfNeeded,\n",
    "    RandomCrop,\n",
    "    CenterCrop\n",
    ")\n",
    "def train_transform(p=1):\n",
    "    return Compose([\n",
    "        PadIfNeeded(min_height=train_crop_height, min_width=train_crop_width, p=1),\n",
    "        RandomCrop(height=train_crop_height, width=train_crop_width, p=1),\n",
    "        VerticalFlip(p=0.5),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        Normalize(p=1)\n",
    "    ], p=p)\n",
    "\n",
    "def val_transform(p=1):\n",
    "    return Compose([\n",
    "        PadIfNeeded(min_height=val_crop_height, min_width=val_crop_width, p=1),\n",
    "        CenterCrop(height=val_crop_height, width=val_crop_width, p=1),\n",
    "        Normalize(p=1)\n",
    "    ], p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_names, val_file_names = get_split()\n",
    "train_loader = make_loader(train_file_names, shuffle=True, transform=train_transform(p=1), problem_type=problem_type,batch_size=batch_size)\n",
    "valid_loader = make_loader(val_file_names, transform=val_transform(p=1), problem_type=problem_type,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "device_ids = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from models import UNet11, LinkNet34, UNet, UNet16, AlbuNet\n",
    "from torch import nn\n",
    "from unetplusplus import UnetPlusPlus \n",
    "\n",
    "model_name = 'LinkNet34'\n",
    "model = LinkNet34(num_classes=num_classes)\n",
    "if torch.cuda.is_available():\n",
    "    model = nn.DataParallel(model, device_ids=device_ids).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import LossBinary, LossMulti\n",
    "jaccard_weight = 0.3\n",
    "loss = LossMulti(num_classes=num_classes, jaccard_weight=jaccard_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import torch.backends.cudnn\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkpoints folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'runs/debug'\n",
    "# root.joinpath('params.json').write_text(json.dumps(vars(args), indent=True, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda(x):\n",
    "    return x.to('cuda', non_blocking=True) if torch.cuda.is_available() else x\n",
    "def write_event(log, step, **data):\n",
    "    data['step'] = step\n",
    "    data['dt'] = datetime.now().isoformat()\n",
    "    log.write(json.dumps(data, sort_keys=True))\n",
    "    log.write('\\n')\n",
    "    log.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, train_loader, valid_loader, validation, init_optimizer, n_epochs=1, lr=0.0001, batch_size=1, fold=None,\n",
    "          num_classes=None, root='runs/debug',model_name=model_name,problem_type=problem_type):\n",
    "    lr = lr\n",
    "    n_epochs = n_epochs\n",
    "    optimizer = init_optimizer(lr)\n",
    "\n",
    "    root = Path(root)\n",
    "    model_path = root / f'{model_name}_{problem_type}.pt'\n",
    "    if model_path.exists():\n",
    "        state = torch.load(str(model_path))\n",
    "        epoch = state['epoch']\n",
    "        step = state['step']\n",
    "        model.load_state_dict(state['model'])\n",
    "        print('Restored model, epoch {}, step {:,}'.format(epoch, step))\n",
    "    else:\n",
    "        epoch = 1\n",
    "        step = 0\n",
    "\n",
    "    save = lambda ep: torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'epoch': ep,\n",
    "        'step': step,\n",
    "    }, str(model_path))\n",
    "\n",
    "    report_each = 10\n",
    "    log = root.joinpath('train.log').open('at', encoding='utf8')\n",
    "    valid_losses = []\n",
    "    for epoch in range(epoch, n_epochs + 1):\n",
    "        model.train()\n",
    "        random.seed()\n",
    "        tq = tqdm.tqdm(total=(len(train_loader) * batch_size))\n",
    "        tq.set_description('Epoch {}, lr {}'.format(epoch, lr))\n",
    "        losses = []\n",
    "        tl = train_loader\n",
    "        mean_loss = 0\n",
    "        for i, (inputs, targets) in enumerate(tl):\n",
    "            inputs = cuda(inputs)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                targets = cuda(targets)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            batch_size = inputs.size(0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            step += 1\n",
    "            tq.update(batch_size)\n",
    "            losses.append(loss.item())\n",
    "            mean_loss = np.mean(losses[-report_each:])\n",
    "            tq.set_postfix(loss='{:.5f}'.format(mean_loss))\n",
    "            if i and i % report_each == 0:\n",
    "                write_event(log, step, loss=mean_loss)\n",
    "        write_event(log, step, loss=mean_loss)\n",
    "        tq.close()\n",
    "        save(epoch + 1)\n",
    "        valid_metrics = validation(model, criterion, valid_loader, num_classes)\n",
    "        write_event(log, step, **valid_metrics)\n",
    "        valid_loss = valid_metrics['valid_loss']\n",
    "        valid_losses.append(valid_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validation import validation_binary, validation_multi\n",
    "valid = validation_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "optimizer = lambda lr: Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, lr 1e-05: 100%|██████████| 1440/1440 [00:58<00:00, 24.68it/s, loss=4.23843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 4.3767, average IoU: 0.4507, average Dice: 0.5201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, lr 1e-05: 100%|██████████| 1440/1440 [00:51<00:00, 27.96it/s, loss=3.96089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 3.9898, average IoU: 0.3315, average Dice: 0.3324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, lr 1e-05: 100%|██████████| 1440/1440 [00:51<00:00, 27.78it/s, loss=2.97181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 3.0765, average IoU: 0.3328, average Dice: 0.3331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, lr 1e-05: 100%|██████████| 1440/1440 [00:51<00:00, 28.11it/s, loss=2.05266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 2.0738, average IoU: 0.3326, average Dice: 0.3330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, lr 1e-05: 100%|██████████| 1440/1440 [00:52<00:00, 27.69it/s, loss=1.54738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 1.5929, average IoU: 0.3313, average Dice: 0.3323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, lr 1e-05: 100%|██████████| 1440/1440 [00:51<00:00, 27.76it/s, loss=1.35804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 1.4054, average IoU: 0.3315, average Dice: 0.3324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, lr 1e-05: 100%|██████████| 1440/1440 [00:51<00:00, 27.76it/s, loss=1.27449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 1.3233, average IoU: 0.3299, average Dice: 0.3316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, lr 1e-05: 100%|██████████| 1440/1440 [00:51<00:00, 27.88it/s, loss=1.23126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 1.2736, average IoU: 0.4842, average Dice: 0.5446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, lr 1e-05: 100%|██████████| 1440/1440 [00:51<00:00, 27.95it/s, loss=1.18833]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 1.2367, average IoU: 0.4857, average Dice: 0.5458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, lr 1e-05: 100%|██████████| 1440/1440 [00:51<00:00, 27.90it/s, loss=1.17054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 1.2110, average IoU: 0.4850, average Dice: 0.5453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, lr 1e-05: 100%|██████████| 1440/1440 [00:51<00:00, 28.05it/s, loss=1.15171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 1.1909, average IoU: 0.4839, average Dice: 0.5453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, lr 1e-05: 100%|██████████| 1440/1440 [00:51<00:00, 28.04it/s, loss=1.10253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 1.1518, average IoU: 0.4845, average Dice: 0.5476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, lr 1e-05: 100%|██████████| 1440/1440 [00:51<00:00, 27.79it/s, loss=1.01027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 1.0492, average IoU: 0.5288, average Dice: 0.6346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, lr 1e-05: 100%|██████████| 1440/1440 [00:51<00:00, 27.98it/s, loss=0.88397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.9337, average IoU: 0.5290, average Dice: 0.6353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, lr 1e-05: 100%|██████████| 1440/1440 [00:50<00:00, 28.29it/s, loss=0.82111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.8803, average IoU: 0.5312, average Dice: 0.6378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, lr 1e-05: 100%|██████████| 1440/1440 [00:51<00:00, 28.05it/s, loss=0.82438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.8464, average IoU: 0.5982, average Dice: 0.7166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, lr 1e-05: 100%|██████████| 1440/1440 [00:51<00:00, 28.11it/s, loss=0.47175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.5092, average IoU: 0.8370, average Dice: 0.9082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, lr 1e-05: 100%|██████████| 1440/1440 [00:51<00:00, 27.96it/s, loss=0.28343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.3451, average IoU: 0.8650, average Dice: 0.9258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, lr 1e-05: 100%|██████████| 1440/1440 [00:51<00:00, 28.14it/s, loss=0.25703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.3134, average IoU: 0.8737, average Dice: 0.9312\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    criterion=loss,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    validation=valid,\n",
    "    init_optimizer=optimizer,\n",
    "    n_epochs=20,\n",
    "    lr=0.00001,\n",
    "    batch_size=batch_size,\n",
    "    num_classes=num_classes,\n",
    "    model_name = model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate mask using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f'predictions/{model_name}/parts'\n",
    "workers = 12\n",
    "from prepare_data import (original_height,\n",
    "                          original_width,\n",
    "                          h_start, w_start\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_transform(p=1):\n",
    "    return Compose([\n",
    "        Normalize(p=1)\n",
    "    ], p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "def predict(model, from_file_names, batch_size, to_path, problem_type, img_transform,workers=workers,):\n",
    "    loader = DataLoader(\n",
    "        dataset=RoboticsDataset(from_file_names, transform=img_transform, mode='predict', problem_type=problem_type),\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=workers,\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (inputs, paths) in enumerate(tqdm.tqdm(loader, desc='Predict')):\n",
    "            inputs = cuda(inputs)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            for i, image_name in enumerate(paths):\n",
    "                if problem_type == 'binary':\n",
    "                    factor = prepare_data.binary_factor\n",
    "                    t_mask = (F.sigmoid(outputs[i, 0]).data.cpu().numpy() * factor).astype(np.uint8)\n",
    "                elif problem_type == 'parts':\n",
    "                    factor = prepare_data.parts_factor\n",
    "                    t_mask = (outputs[i].data.cpu().numpy().argmax(axis=0) * factor).astype(np.uint8)\n",
    "                elif problem_type == 'instruments':\n",
    "                    factor = prepare_data.instrument_factor\n",
    "                    t_mask = (outputs[i].data.cpu().numpy().argmax(axis=0) * factor).astype(np.uint8)\n",
    "\n",
    "                h, w = t_mask.shape\n",
    "\n",
    "                full_mask = np.zeros((original_height, original_width))\n",
    "                full_mask[h_start:h_start + h, w_start:w_start + w] = t_mask\n",
    "\n",
    "                instrument_folder = Path(paths[i]).parent.parent.name\n",
    "\n",
    "                (to_path / instrument_folder).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "                cv2.imwrite(str(to_path / instrument_folder / (Path(paths[i]).stem + '.png')), full_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num file_names = 360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict: 100%|██████████| 45/45 [00:17<00:00,  2.53it/s]\n"
     ]
    }
   ],
   "source": [
    "val_file_names\n",
    "print('num file_names = {}'.format(len(val_file_names)))\n",
    "output_path = Path(output_path)\n",
    "output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "predict(model, val_file_names, batch_size, output_path, problem_type=problem_type,img_transform=img_transform(p=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(y_true, y_pred):\n",
    "    intersection = (y_true * y_pred).sum()\n",
    "    union = y_true.sum() + y_pred.sum() - intersection\n",
    "    return (intersection + 1e-15) / (union + 1e-15)\n",
    "\n",
    "\n",
    "def dice(y_true, y_pred):\n",
    "    return (2 * (y_true * y_pred).sum() + 1e-15) / (y_true.sum() + y_pred.sum() + 1e-15)\n",
    "\n",
    "def general_dice(y_true, y_pred):\n",
    "    result = []\n",
    "\n",
    "    if y_true.sum() == 0:\n",
    "        if y_pred.sum() == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    for instrument_id in set(y_true.flatten()):\n",
    "        if instrument_id == 0:\n",
    "            continue\n",
    "        result += [dice(y_true == instrument_id, y_pred == instrument_id)]\n",
    "\n",
    "    return np.mean(result)\n",
    "\n",
    "def general_jaccard(y_true, y_pred):\n",
    "    result = []\n",
    "\n",
    "    if y_true.sum() == 0:\n",
    "        if y_pred.sum() == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    for instrument_id in set(y_true.flatten()):\n",
    "        if instrument_id == 0:\n",
    "            continue\n",
    "        result += [jaccard(y_true == instrument_id, y_pred == instrument_id)]\n",
    "\n",
    "    return np.mean(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from prepare_data import height, width, h_start, w_start\n",
    "\n",
    "target_path = f'predictions/{model_name}'\n",
    "train_path = 'data/cropped_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:54<00:00,  6.84s/it]\n"
     ]
    }
   ],
   "source": [
    "result_dice = []\n",
    "result_jaccard = []\n",
    "if problem_type == 'binary':\n",
    "    for instrument_id in tqdm.tqdm(range(1, 9)):\n",
    "        instrument_dataset_name = 'instrument_dataset_' + str(instrument_id)\n",
    "\n",
    "        pred_folder_name = (Path(target_path) / 'binary' / instrument_dataset_name)\n",
    "        if not os.path.exists(pred_folder_name):\n",
    "            continue\n",
    "\n",
    "        for file_name in (Path(train_path) / instrument_dataset_name / 'binary_masks').glob('*'):\n",
    "            pred_file_name = (Path(target_path) / 'binary' / instrument_dataset_name / file_name.name)\n",
    "            if not os.path.exists(pred_file_name):\n",
    "                continue\n",
    "            \n",
    "            y_true = (cv2.imread(str(file_name), 0) > 0).astype(np.uint8)\n",
    "\n",
    "            pred_image = (cv2.imread(str(pred_file_name), 0) > 255 * 0.5).astype(np.uint8)\n",
    "            y_pred = pred_image[h_start:h_start + height, w_start:w_start + width]\n",
    "\n",
    "            result_dice += [dice(y_true, y_pred)]\n",
    "            result_jaccard += [jaccard(y_true, y_pred)]\n",
    "            \n",
    "elif problem_type == 'parts':\n",
    "    for instrument_id in tqdm.tqdm(range(1, 9)):\n",
    "        instrument_dataset_name = 'instrument_dataset_' + str(instrument_id)\n",
    "        for file_name in (\n",
    "                Path(train_path) / instrument_dataset_name / 'parts_masks').glob('*'):\n",
    "            y_true = cv2.imread(str(file_name), 0)\n",
    "\n",
    "            pred_file_name = Path(target_path) / 'parts' / instrument_dataset_name / file_name.name\n",
    "            if not os.path.exists(pred_file_name):\n",
    "                continue\n",
    "\n",
    "            y_pred = cv2.imread(str(pred_file_name), 0)[h_start:h_start + height, w_start:w_start + width]\n",
    "\n",
    "            result_dice += [general_dice(y_true, y_pred)]\n",
    "            result_jaccard += [general_jaccard(y_true, y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice =  0.8510731883170084 0.11170368774976065\n",
      "Jaccard =  0.7659906502225586 0.12557047251563083\n"
     ]
    }
   ],
   "source": [
    "print('Dice = ', np.mean(result_dice), np.std(result_dice))\n",
    "print('Jaccard = ', np.mean(result_jaccard), np.std(result_jaccard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
